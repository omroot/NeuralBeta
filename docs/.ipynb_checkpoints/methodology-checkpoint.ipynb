{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1473c0a-07f7-4f22-897a-de22ef699629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126b8a3-2bb1-4cde-a070-f97b14088f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1419a1ad-270f-42b3-9035-dc0a6e95fc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4ba0df2-9cea-4367-a9ec-8ed309556740",
   "metadata": {},
   "source": [
    "- $ y_{i,t}$: The target asset return for asset $ i $ at time $ t $.\n",
    "     - $i \\in [m] $: Index for the target assets, where $[m]$ denotes the set $\\{1, 2, \\ldots, m\\}$.\n",
    "     -  $t \\in T $: Time index, where $ T $ represents the set of time periods.\n",
    "- $ x_{i,t} \\in \\mathbb{R}^d $: The explanatory variables for asset $ i $ at time $ t $.\n",
    "     - $ d $: Dimensionality of the explanatory variables.\n",
    "- $ D_{i,s,t} = (x_{i,s:t}, y_{i,s:t}) $: The dataset for asset $ i $ from time $ s $ to time $ t $. This includes all observations of explanatory variables $ x_{i,s:t} $ and target values $ y_{i,s:t}$ within the interval $[s, t]$.\n",
    "-  $ D_i = \\cup_{s < t} D_{i,s,t} = \\left(\\cup_{s < t} x_{i,s:t}, \\cup_{s < t} y_{i,s:t}\\right) $: The cumulative dataset for asset $ i $ up to time $ t $. This includes all data for explanatory variables and target values from times before $ t $.\n",
    "- $ \\hat{\\beta}_{i,t+1} $: The predicted coefficients for asset $ i $ at time $ t+1 $.\n",
    "     - The functional form is given by $ \\hat{\\beta}_{i,t+1} = f_i(t, D_i) $, where $ f_i $ is a function learned from the dataset $ D_i$ and time $ t $.\n",
    "-  $ f_i $: The function representing the model for asset $ i$. In the context of the formulation, this function is learned from the dataset $ D_i $ and used to estimate $ \\hat{\\beta}_{i,t+1} $.\n",
    "-  $ \\tau_{i,t} $: A weighting factor or term for asset $ i $ at time $ t $. It may represent the frequency or importance of the observations.\n",
    "- $ T_n $: A subset of time periods used for training or validation. Specifically, $ T_n \\subset T $ indicates the set of time periods considered for the training or validation phase in the objective function.    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d907e-7183-464a-9198-27655d30662f",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "- $ \\langle \\hat{\\beta}_{i,t+1}, x_{i,t+1} \\rangle $: The inner product or prediction for asset $ i $ at time $ t+1 $, calculated using the coefficients $ \\hat{\\beta}_{i,t+1} $ and explanatory variables $ x_{i,t+1} $.\n",
    "    \n",
    "- Objective Function:\n",
    "    $$\n",
    "    \\min_{(f_i)_{i \\in [m]}} \\frac{1}{nm} \\sum_{t \\in T_n} \\sum_{i \\in [m]}   L \\left(y_{i,t+1}, \\langle \\hat{\\beta}_{i,t+1}, x_{i,t+1} \\rangle \\right)\n",
    "    $$\n",
    "    -  $ n $: Number of time periods.\n",
    "    - $ m $: Number of target assets.\n",
    "    - The objective function aims to minimize the average loss across all assets and time periods.\n",
    "- $ L $: The loss function used in training. In this case, $ L $ is the mean squared error (MSE), defined as:\n",
    "   $$\n",
    "    L \\left(y_{i,t+1}, \\langle \\hat{\\beta}_{i,t+1}, x_{i,t+1} \\rangle \\right) = \\left(y_{i,t+1} - \\langle \\hat{\\beta}_{i,t+1}, x_{i,t+1} \\rangle \\right)^2\n",
    "   $$\n",
    "    where $ \\langle \\hat{\\beta}_{i,t+1}, x_{i,t+1} \\rangle $ denotes the predicted return for asset $ i $ at time $ t+1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd522c91-4413-4cf3-a828-f334b29223bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c423f40-5cee-4d64-92d4-9af2983f87f6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}(y_{t+1}, \\langle \\beta_{t+1}, x_{t+1}\\rangle )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731d94f-640e-4724-9181-64bf617c3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\documentclass{article}\n",
    "\\usepackage{amsmath, amssymb}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\section*{Notation}\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item \\( y_{i,t} \\): The target asset return for asset \\( i \\) at time \\( t \\).\n",
    "    \\begin{itemize}\n",
    "        \\item \\( i \\in [m] \\): Index for the target assets, where \\([m]\\) denotes the set \\(\\{1, 2, \\ldots, m\\}\\).\n",
    "        \\item \\( t \\in T \\): Time index, where \\( T \\) represents the set of time periods.\n",
    "    \\end{itemize}\n",
    "    \n",
    "    \\item \\( x_{i,t} \\in \\mathbb{R}^d \\): The explanatory variables for asset \\( i \\) at time \\( t \\).\n",
    "    \\begin{itemize}\n",
    "        \\item \\( d \\): Dimensionality of the explanatory variables.\n",
    "    \\end{itemize}\n",
    "    \n",
    "    \\item \\( D_{i,s,t} = (x_{i,s:t}, y_{i,s:t}) \\): The dataset for asset \\( i \\) from time \\( s \\) to time \\( t \\). This includes all observations of explanatory variables \\( x_{i,s:t} \\) and target values \\( y_{i,s:t} \\) within the interval \\([s, t]\\).\n",
    "    \n",
    "    \\item \\( D_i = \\cup_{s < t} D_{i,s,t} = \\left(\\cup_{s < t} x_{i,s:t}, \\cup_{s < t} y_{i,s:t}\\right) \\): The cumulative dataset for asset \\( i \\) up to time \\( t \\). This includes all data for explanatory variables and target values from times before \\( t \\).\n",
    "    \n",
    "    \\item \\( \\hat{\\beta}_{i,t+1} \\): The predicted coefficients for asset \\( i \\) at time \\( t+1 \\).\n",
    "    \\begin{itemize}\n",
    "        \\item The functional form is given by \\( \\hat{\\beta}_{i,t+1} = f_i(t, D_i) \\), where \\( f_i \\) is a function learned from the dataset \\( D_i \\) and time \\( t \\).\n",
    "    \\end{itemize}\n",
    "    \n",
    "    \\item \\( f_i \\): The function representing the model for asset \\( i \\). In the context of the formulation, this function is learned from the dataset \\( D_i \\) and used to estimate \\( \\hat{\\beta}_{i,t+1} \\).\n",
    "    \n",
    "    \\item \\( \\tau_{i,t} \\): A weighting factor or term for asset \\( i \\) at time \\( t \\). It may represent the frequency or importance of the observations.\n",
    "    \n",
    "    \\item \\( L \\): The loss function used in training. In this case, \\( L \\) is the mean squared error (MSE), defined as:\n",
    "    \\[\n",
    "    L \\left(y_{i,t+1}, \\langle \\hat{\\beta}_{i,t+1}, x_{i,t+1} \\rangle \\right) = \\left(y_{i,t+1} - \\langle \\hat{\\beta}_{i,t+1}, x_{i,t+1} \\rangle \\right)^2\n",
    "    \\]\n",
    "    where \\( \\langle \\hat{\\beta}_{i,t+1}, x_{i,t+1} \\rangle \\) denotes the predicted return for asset \\( i \\) at time \\( t+1 \\).\n",
    "    \n",
    "    \\item \\( \\langle \\hat{\\beta}_{i,t+1}, x_{i,t+1} \\rangle \\): The inner product or prediction for asset \\( i \\) at time \\( t+1 \\), calculated using the coefficients \\( \\hat{\\beta}_{i,t+1} \\) and explanatory variables \\( x_{i,t+1} \\).\n",
    "    \n",
    "    \\item Objective Function:\n",
    "    \\[\n",
    "    \\min_{(f_i)_{i \\in [m]}} \\frac{1}{nm} \\sum_{t \\in T_n} \\sum_{i \\in [m]} \\tau_{i,t} L \\left(y_{i,t+1}, \\langle \\hat{\\beta}_{i,t+1}, x_{i,t+1} \\rangle \\right)\n",
    "    \\]\n",
    "    \\begin{itemize}\n",
    "        \\item \\( n \\): Number of time periods.\n",
    "        \\item \\( m \\): Number of target assets.\n",
    "        \\item The objective function aims to minimize the average loss across all assets and time periods.\n",
    "    \\end{itemize}\n",
    "\\end{enumerate}\n",
    "\n",
    "\\end{document}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff0c47-3382-4299-a358-f717fb71474a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333a88b-1779-4d79-bcec-77b523824c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5035c5b-493f-456f-b64b-54b91c52e604",
   "metadata": {},
   "source": [
    "| **Scenario**             | **Constant  $\\beta$**                                                                                                           | **Stepwise 𝛽**                                                                                                           | **Cyclical $\\beta$**                                                                                                           |\n",
    "|--------------------------|--------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Description**           | Simplest case: time-invariant relation between response and explanatory variable.                                          | Time-varying beta with regime shifts, where $\\beta$ stays constant for a period and then jumps to a new level.                   | Cyclical patterns in financial time series (e.g., seasonality, business cycles).                                           |\n",
    "| **Ground Truth $\\beta$**        | $\\beta_t =c, c  \\sim \\mathcal{N}(1, 1)$                                                                                                      | $\\beta_t$ is stepwise with jumps generated from $\\mathcal{N}(1, 1)$.                                                                         | $\\beta_t = \\sin(\\beta_0 + ct), \\beta_0 \\sim \\mathcal{N}(0, 1), c \\sim \\mathcal{U}(4, 32)$                                                                                     |\n",
    "| **Model Evaluation**      | Test whether NeuralBeta model converges to the optimal solution derived via Bayesian linear regression.                    | Test NeuralBeta’s ability to adapt to sudden changes (market regime shifts).                                               | Test NeuralBeta’s ability to capture cyclical patterns without further modification.                                        |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22922f14-f149-4974-8d64-2219e7e805fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8bd58-aa41-42ec-bab6-b6056b0b5885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6d9cdad-fdb9-4bf7-98bd-309dd2faf99e",
   "metadata": {},
   "source": [
    "\n",
    "Synthetic time series are generated for each scenario: \n",
    "$$\n",
    "x_t \\sim  t_{10} (0, 1), y_t = \\beta_t \\times x_t + \\epsilon_t, \\text{ with } \\epsilon_t \\sim  \\mathcal{N}(0, 1)\n",
    "$$\n",
    "The length is 65, with 100,000 samples. Use 70% of the data for training, 20% for validation, and 10% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a12f2e-bf00-424c-a247-ef05489c9088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df0927-3d23-4cc7-b6f9-4dc8cb6ce335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdde654-3c02-4ef3-bb4c-06adc1c067f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "029babd0-7928-4183-aaf1-0699cac7465e",
   "metadata": {},
   "source": [
    "- **Streaming Data Handling**: Conventional methods such as ordinary least squares (OLS) or rolling OLS operate on static datasets or a fixed window of past data. In contrast, our hedging task requires progressively updating the dataset as new data points become available. Existing methods often fail to adequately handle streaming data, where the dataset evolves dynamically.\n",
    "\n",
    "- **Time-Varying Coefficients**: Traditional OLS assumes time-homogeneous coefficients, which can be inappropriate for real-world financial markets where relationships between variables change over time. In our problem, the ground truth  $\\beta$  is not static, and the coefficient estimation must reflect this time-varying nature.\n",
    "\n",
    "- **Lookback Window Selection**: Rolling OLS uses a fixed lookback window \n",
    "$h$,  and choosing the window size can be arbitrary. This approach risks misestimating both long-term and short-term data relevance by treating all points within the window equally while disregarding older data entirely.\n",
    "\n",
    "- **Weighting Schemes in WLS**: While weighted least squares (WLS) introduces a dynamic weighting scheme to adjust the importance of data points, configuring proper weighting schemes (e.g., exponential or power law weights) across all time intervals remains challenging. This introduces complexity, additional parameters to tune, and potential misalignment with real-world data behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39601f3-14d6-4d19-a145-36c8c08e82e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a837155-7274-46a2-b000-66b1a7a560f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0ff1321-f413-45b4-8ffb-16b39ba3a657",
   "metadata": {},
   "source": [
    "We begin with a simple scenario where one hedges a single target asset against multiple hedging instruments on a daily basis. The objective is to determine the optimal hedging ratio of each instrument that minimizes the next day's hedging error, ultimately achieving a low average hedging error over the entire horizon. Once this single-asset hedging scenario is formalized, it scales naturally to the multi-asset case and can be applied to other prediction tasks.\n",
    "\n",
    "Let $ T = \\{0, 1, 2, \\dots\\} $ denote the discrete time index for data of a certain frequency (e.g., daily). For a time interval $ (s, t] $ where $ s, t \\in T $ and \\( s < t \\), define the dataset $ D_{s,t} = \\{(x_{s+1}, y_{s+1}), (x_{s+2}, y_{s+2}), \\dots, (x_t, y_t)\\} $, where $ x \\in \\mathbb{R}^d $ is the explanatory variable (e.g., factor returns) and $ y \\in \\mathbb{R} $ is the scalar response variable (e.g., a single stock return).\n",
    "\n",
    "At any time $t \\in T $, we assume a linear relationship between $ x_t $ and $ y_t$ with coefficient $ \\beta_t \\in \\mathbb{R}^d $ and noise $ \\epsilon_t \\in \\mathbb{R} $, described by the model:\n",
    "\n",
    "$$\n",
    "y_t = \\langle \\beta_t, x_t \\rangle + \\epsilon_t \\tag{1}\n",
    "$$\n",
    "\n",
    "where $ \\langle \\cdot, \\cdot \\rangle $ denotes the inner product.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3840808-74f6-49ce-8296-42436f77e7f1",
   "metadata": {},
   "source": [
    "A one-step hedging task at time $ t $ involves determining the optimal hedging ratio $ \\hat{\\beta}_{t+1} $ given the available data $ D_{0,t}$ such that the ex-ante hedging error at time $ t+1$ is minimized:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{t+1} = \\underset{\\beta}{\\text{argmin}} \\, L(y_{t+1}, \\langle \\beta, x_{t+1} \\rangle)\n",
    "$$\n",
    "\n",
    "where $ L$ is a risk measure (e.g., expected quadratic loss or negative log-likelihood). The optimal hedging ratio $\\hat{\\beta}_{t+1} $ is assumed to be inferable from the dataset $ D_{0,t} $, taking the form:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{t+1} = f(t, D_{0,t}) \\tag{2}\n",
    "$$\n",
    "\n",
    "for some function $ f : T \\times D \\to \\mathbb{R}^d $, which could be time-inhomogeneous.\n",
    "\n",
    "Consider a practical scenario over an $ n $-day horizon starting at day $ \\tau \\in T $, denoted $ T^n_\\tau = \\{\\tau, \\tau+1, \\dots, \\tau+n-1\\} $ with $ \\tau \\geq h $ (for some chosen lookback window $h \\in \\mathbb{N} $). The one-step hedge ratio prediction (Equation 2) is performed each day $ t \\in T^n_\\tau $, with the objective of finding the function $ f $ that minimizes the average hedging error over the entire horizon:\n",
    "\n",
    "$$\n",
    "\\underset{f}{\\text{min}} \\, \\frac{1}{n} \\sum_{t \\in T^n_\\tau} L(y_{t+1}, \\langle \\hat{\\beta}_{t+1}, x_{t+1} \\rangle)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5ae89-1ef4-4214-b5b1-1ea158fcea4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdc3c6-5606-48a1-94bc-dcd0005b5828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "498b8908-efcd-4c62-9652-85ce06051d97",
   "metadata": {},
   "source": [
    "### 3.2 Market Data\n",
    "\n",
    "| **Aspect**                       | **Details**                                                                                                                         |\n",
    "|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Time Horizon**                 | January 1, 2010, to December 31, 2023                                                                                                 |\n",
    "| **Data Used**                    | - Daily return series of the S&P 500 index <br> - Size factor index <br> - Value factor index <br> - S&P 500 components             |\n",
    "| **Data Segmentation**            | - Training Period: January 1, 2010, to December 31, 2017 <br> - Validation Period: January 1, 2018, to December 31, 2019 <br> - Test Period: January 1, 2020, to December 31, 2023 |\n",
    "| **Component Tracking**           | A fixed snapshot of S&P 500 components as of May 1, 2024, with price histories from January 1, 2010, resulting in 468 stocks.     |\n",
    "| **Experiment Scenarios**         | - **Univariate Scenario:** CAPM $\\beta$ for each S&P 500 stock <br> - **Multivariate Scenario:** Factor $\\beta$ for market, size, and value factors (similar to Fama-French three-factor model)  |\n",
    "| **S&P 500 Components with CAPM** | **Description:** Uses daily return series of S&P 500 index and individual components <br> **Results:** Performance shown in \"Univariate\" entry in Table 1 |\n",
    "| **S&P 500 Components with Factors** | **Description:** Multivariate beta estimation using SPX, size, and value indices <br> **Results:** Performance shown in \"Multivariate\" entry in Table 1 <br> **Findings:** Interpretable architecture with Attention performs best; NeuralBeta models slightly worse than the benchmark |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404f98c-7820-4f6a-9f98-b5066c1c3db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-labs",
   "language": "python",
   "name": "dl-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
